## 선형회귀
 - 선형회귀(Linear Regression)
 - 종속변수 y와 한 개 이상의 독립변수 x와의 선형 상관관계를 모델링하는 회귀분석 기법
 - 기존 데이터를 활용해 연속형 변수값을 예측
 - y = ax+b꼴의 수식을 만들고 a와 b의 값을 찾아냄
 - 하나 이상의 특성과 연속적인 타깃 변수 사이의 관계를 모델링하는 것이 목적
 - 지도 학습의 회귀는 범주형 클래스 레이블이 아니라 연속적인 출력 값을 예측
 - 단순 선형 회귀
    - 단순 선형 회귀는 하나의 특성(설명 변수(explanatory variable) x)과 연속적인 타깃(응답 변수 (response variable) y) 사이의 관계를 모델링
 - 특성이 하나인 선형 모델 공식은 다음과 같음
 - 단순 선형 회귀
    - 여기서 w0는 y축 절편을 나타내고 w1은 특성의 가중치
    - 특성과 타깃 사이의 관계를 나타내는 선형 방정식의 가중치를 학습하는 것이 목적
    - 이 방정식으로 훈련 데이터셋이 아닌 새로운 샘플의 타깃 값을 예측할 수 있음
 - 단순 선형 회귀
    - 데이터에 가장 잘 맞는 이런 직선을 회귀 직선(regression line)이라고도 함
    - 회귀 직선과 훈련 샘플 사이의 직선 거리를 오프셋(offset) 또는 예측 오차인 잔차(residual)라고 함
 - 예측 함수와 실제값 간의 차이
    - 예측 함수는 예측값과 실제값 간의 차이를 최소화하는 방향으로
    - 데이터 n개 중 i번째 데이터의 y값에 대한 실제값과 예측값의 차이
    - 데이터가 5개 있을 때 5개 데이터의 오차의 합
    - 오차 값들이 음수와 양수로 나왔을 때 값들 간의 차이가 상쇄되어 0으로 계산될 수 있음
 - 값의 제곱을 사용하여 오차의 합을 표현
 - 같은 식을 행렬로 표현
 - 제곱 오차(square error) : 예측값과 실제값의 제곱을 표시하여 오차를 나타냄
 - 1. 비용함수의 개념
 - 비용함수(cost function)
    - 머신러닝에서 최소화해야 할 예측값과 실제값의 차이
 - 가설함수(hypothesis function)
    - 예측값을 예측하는 함수
 - 함수 입력값은 x이고 함수에서 결정할 것은 θ로, 가중치(weight) 값인 wn
 - 비용함수가 두 개의 가중치 값으로 결정됨
    - 잔차의 제곱합(Error sum of squares) : 예측값인 가설함수와 실제값인 값 간의 차이를 제곱해서 모두 합함
        총 데이터는 m개가 존재하고 각 데이터의 예측값과 실제값을 뺀 후 제곱한 값들을 모두 합한 값
    - 손실함수(loss function) : 비용함수에서 잔차의 제곱합 부분
    - 평균 제곱 오차(mean squared error, MSE) : 잔차의 제곱합을 2m으로 나눈 값
 - 비용함수를 수식의 목적이 나타나도록 표기
 - 비용함수의 편미분
    - 비용함수식의 최적값 = 아래 수식 값의 최솟값
 - 미분법
    - Σ 기호는 신경 쓰지 않고 미분
    - 제곱 꼴의 미분은 (…)² 꼴을 먼저 미분하고 괄호 안을 미분
 - 최소자승법(least square method)
    - 선형대수의 표기법을 사용하여 방정식으로 선형회귀 문제를 푸는 방법
 - 비용함수를 미분
    - 미분한 값을 0으로 만드는 최적값을 구한다
 - 최소자승법의 활용
    - 데이터의 개수가 피쳐의 개수보다 많은 경우가 대부분이라서 자주 사용됨
    - 최소자승법의 장점 : 반복(iteration)과 사용자가 지정하는 하이퍼 매개변수(hyper parameter)가 존재하지 않아서 데이터만 있으면 쉽게 해를 구할 수 있음
    - 단점 : 피쳐가 늘어나면 속도가 느려짐
        현재 컴퓨터의 연산속도로는 다른 알고리즘에 비해 느린 것이 아님
 - 경사하강법(gradient descent)
    - 경사를 하강하면서 수식을 최소화하는 매개변수의 값을 찾아내는 방법
    - 점이 최솟값을 달성하는 방향으로 점점 내려감
        몇 번 적용할 것인가? : 많이 실행할수록 최솟값에 가까워짐
        한 번에 얼마나 많이 내려갈 것인가? : 한 번에 얼마나 많은 공간을 움직일지를 기울기, 즉 경사라고 부름
    - 경사(gradient)
        경사하강법의 하이퍼 매개변수
 - 경사하강법에서 개발자가 결정해야 할 것
    - 학습률(learning rate)을 얼마로 할 것인가? α 값을 결정
        반복이 수행될 때마다 최솟값 변화
        값이 너무 작으면 충분히 많은 반복을 적용해도 원하는 최적값을 찾지 못하는 경우 발생
        값이 너무 크면 발산하여 최솟값 수렴 않거나 시간이 너무 오래 걸림
    - 얼마나 많은 반복(iteration)으로 돌릴 것인가?
        반복 횟수가 충분하지 않다면 최솟값을 찾지 못하는 경우 발생
        반복 횟수가 너무 많다면 필요 없는 시간을 허비할 수도 있음
 - 경사하강법으로 선형회귀
    - J를 최소화하는 방향으로 학습을 실행하기
 - w변수가 두 개이기 때문에 3차원 그래프로 J를 표현
 - 훈련/테스트 분할(train/test split)
    - 머신러닝에서 데이터를 학습을 하기 위한 학습 데이터셋(train dataset)과 학습의 결과로 생성된 모델의 성능을 평가하기 위한 테스트 데이터셋(test dataset)으로 나눔
    - 모델이 새로운 데이터셋에도 일반화(generalize)하여 처리할 수 있는지를 확인
 - 모델이 데이터에 과다적합(over-fit)된 경우
    - 생성된 모델이 특정 데이터에만 잘 맞아서 해당 데이터셋에 대해서는 성능을 발휘할 수 있지만 새로운 데이터셋에서는 전혀 성능을 낼 수 없다
 - 모델이 데이터에 과소적합(under-fit)된 경우
    - 기존 학습 데이터를 제대로 예측하지 못함
 - 홀드아웃 메서드(hold-out method)
    - 전체 데이터셋에서 일부를 학습 데이터와 테스트 데이터로 나누는 일반적인 데이터 분할 기법
        전체 데이터에서 랜덤하게 학습 데이터셋과 테스트 데이터셋을 나눔
        일반적으로 7:3 또는 8:2 정도의 비율
        sklearn 모듈이 제공하는 train_test_split 함수
 - 선형회귀의 성능 측정 지표
    - MAE(Mean Absolute Error) : 평균 절대 잔차
    - RMSE(Root Mean Squared Error) : 평균제곱근 오차
    - 결정계수(R-squared) : 두 개의 값의 증감이 얼마나 일관성을 가지는지 나타내는 지표
 - MAE(Mean Absolute Error)
    - 평균 절대 잔차
    - 모든 테스트 데이터에 대해 예측값과 실제값의 차이에 대해 절댓값을 구하고, 이 값을 모두 더한 후에 데이터의 개수만큼 나눈 결과
 - 직관적으로 예측값과 실측값의 차이를 알 수 있음
    - sklearn 모듈에서는 median_absolute_error 함수
 - RMSE(Root Mean Squared Error)
    - 평균제곱근 오차
    - 오차에 대해 제곱을 한 다음 모든 값을 더하여 평균을 낸 후 제곱근을 구함
    - MAE에 비해 상대적으로 값의 차이가 더 큼
    - 차이가 크게 나는 값에 대해서 페널티를 주고 싶다면 RMSE 값을 사용
    - sklearn 모듈에서 RMSE를 직접적으로 지원하지는 않고 mean_squared_error 함수활용 간접지원 및 squared=False
1. from math import sqrt
sqrt(mean_squared_error(y_true, y_pred))
2. mean_squared_error(y_true, y_pred, squared=False)
 - 결정계수(R-squared)
    - 두 개의 값의 증감이 얼마나 일관성을 가지는지 나타내는 지표
    - 예측값이 크면 클수록 실제값도 커지고, 예측값이 작으면 실제값도 작아짐
    - 두 개의 모델 중 어떤 모델이 조금 더 상관성이 있는지를 나타낼 수 있지만, 값의 차이 정도가 얼마인지는 나타낼 수 없다는 한계가 있음
    - sklearn 모듈에서 r2_score 사용
 - 경사하강법 종류
    - 전체-배치 경사하강법(full-batch gradient descent)
    - 확률적 경사하강법(Stochastic Gradient Decent, SGD)
    - 미니-배치 경사하강법(mini-batch gradient descent)
 - 전체-배치 경사하강법(full-batch gradient descent)
    - 모든 데이터를 한 번에 입력하는 경사하강법
        배치(batch) : 하나의 데이터셋
    - 하나의 값에 대한 경사도를 구한 다음 값들을 업데이트
 - 실제로는 각 데이터의 경사도를 모두 더해 하나의 값으로 가중치를 업데이트
 - 점 한 개씩 사용하여 가중치를 업데이트하지 않는 이유
    - 시간이 오래 걸림
    - 시작점에 따라 지역 최적화(local optimum)에 빠짐
        그래프 전체에서 최솟점을 찾지 못하고 부분최솟점에 최적점이 위치
 - 전체-배치 경사하강법의 특징
    - 업데이트 횟수 감소
        가중치 업데이트 횟수가 줄어 계산상 효율성 상승
    - 안정적인 비용함수 수렴
        모든 값의 평균을 구하기 때문에 일반적으로 경사하강법이 갖는 지역 최적화 문제를 만날 가능성 도 있음
    - 업데이트 속도 증가
        대규모 데이터셋을 한 번에 처리하면 모델의 매개변수 업데이트 속도에 문제 발생이 적어짐
        데이터가 백만 단위 이상을 넘어가면 하나의 머신에서는 처리가 불가능해져서 메모리 문제가 발생
 - 확률적 경사하강법(Stochastic Gradient Decent, SGD)
    - 학습용 데이터에서 샘플들을 랜덤하게 뽑아서 사용
    - 대상 데이터를 섞은(shuffle) 후, 일반적인 경사하강법처럼 데이터를 한 개씩 추출하여 가중치 업데이트
 - SGD의 장점
    - 빈번한 업데이트를 하기 때문에 데이터 분석가가 모델의 성능 변화를 빠르게 확인
    - 데이터의 특성에 따라 훨씬 더 빠르게 결과값을 냄
    - 지역 최적화를 회피
 - SGD의 단점
    - 대용량 데이터를 사용하는 경우 시간이 매우 오래 걸림
    - 결과의 마지막 값을 확인하기 어려움
    - 흔히 ‘튀는 현상’이라고 불리는데 비용함수의 값이 줄어들지 않고 계속 변화할 때 정확히 언제 루프(loop)가 종료되는지 알 수 없어 판단이 어렵다
 - 미니-배치 경사하강법(mini-batch gradient descent) 또는 미니-배치 SGD(mini-batchSGD)
    - 데이터의 랜덤한 일부분만 입력해서 경사도 평균을 구해 가중치 업데이트
 - 에포크(epoch)
    - 데이터를 한 번에 모두 학습시키는 횟수
    - 전체-배치 SDG를 한 번 학습하는 루프가 실행될 때 1에포크의 데이터가 학습된다고 말함
 - 배치 사이즈(batch-size)
    - 한 번에 학습되는 데이터의 개수
    - 총 데이터가 5012개 있고 배치 사이즈가 512라면 10번의 루프가 돌면서 1에포크를 학습했다고 말함
 - 에포크와 배치 사이즈는 하이퍼 매개변수이므로 데이터 분석가가 직접 선정함
 - 과대적합(overfitting)
    - 머신러닝 모델을 학습할 때 학습 데이터셋에 지나치게 최적화하여 발생하는 문제
    - 모델을 지나치게 복잡하게 학습하여 학습 데이터셋에서는 모델 성능이 높게 나타나지만 정작 새로운 데이터가 주어졌을 때 정확한 예측/분류를 수행하지 못함
 - 과소적합(underfitting)
    - 과대적합의 반대 개념으로서 머신러닝 모델이 충분히 복잡하지 않아(최적화가 제대로 수행되지 않아) 학습 데이터의 구조/패턴을 정확히 반영하지 못하는 문제
 - 편향(bias)
    - 학습된 모델이 학습 데이터에 대해 만들어 낸 예측값과 실제값과의 차이
    - 모델의 결과가 얼마나 한쪽으로 쏠려 있는지 나타냄
    - 편향이 크면 학습이 잘 진행되기는 했지만 해당 데이터에만 잘 맞음
 - 분산(variance)
    - 학습된 모델이 테스팅 데이터에 대해 만들어 낸 예측값과 실제값과의 차이
    - 모델의 결과가 얼마나 퍼져 있는지 나타냄
 - 편향-분산 트레이드오프(bias-variance trade-off)
    - 편향과 분산의 상충관계
 - 과대적합이 발생할 때 경사하강법 루프가 진행될수록 학습 데이터셋에 대한 비용함수의 값은 줄어들지만 테스트 데이터셋의 비용함수 값은 증가
 - 선형회귀 외에도 결정트리(decision tree)나 딥러닝처럼 연산에 루프가 필요한 모든 알고리즘에서 똑같이 발생
 - 오컴의 면도날 원리
    - ‘보다 적은 수의 논리로 설명이 가능한 경우, 많은 수의 논리를 세우지 않는다’
        ‘경제성의 원리’ 또는 ‘단순성의 원리’
        머신러닝에서는 너무 많은 피쳐를 사용하지 않는 것
 - 선형회귀에서 과대적합 해결책
    - 더 많은 데이터 활용하기 : 오류가 없고, 분포가 다양한 데이터를 많이 확보
    - 피쳐의 개수 줄이기 : 필요한 피쳐만 잘 찾아 사용
    - 적절한 매개변수 선정하기 : SGD의 학습률이나 루프의 횟수처럼 적절한 하이퍼 매개변수를 선정
    - 정규화 적용하기 : 데이터 편향성에 따라 필요 이상으로 증가한 피쳐의 가중치 값을 적절히 줄이는 규제 수식을 추가
 - 리지 회귀(ridge regression)
    - L2 정규화(L2 regularization)
    - 놈(norm)
    - 좌표평면의 원점에서 점까지의 거리를 나타내어 벡터의 크기를 측정하는 기법
    - X는 하나의 벡터
    - L2 놈(L2 norm) : 벡터 각 원소들의 제곱합에 제곱근을 취함
 - 리지 회귀는 L2 놈을 선형회귀의 비용함수 수식에 적용
 - 뒷부분에 새로 붙인 수식은 페널티텀(penalty term)으로, 모델의 가중치 값들의 제곱의합
    - 가중치 값이 조금이라도 커질 때 비용함수에 매우 큰 영향
    - λ가 클수록 전체 페널티텀의 값이 커져 θ 값이 조절됨
    - λ는 사람이 직접 값을 입력하는 하이퍼 매개변수
 - 리지 회귀 수식을 미분하면 j의 값이 1 이상일 때 페널티가 적용됨
 - 라쏘 회귀(lasso regression)
    - L1 정규화(L1 regularization)라고 부름
    - 가중치에 페널티텀을 추가하는데, 기존 수식에다 L1 놈 페널티를 추가하여 계산
 - L1 놈(L1 norm) : 절대값을 사용하여 거리를 측정
 - L1 정규화와 L2 정규화가 실제 적용되는 과정
 - 타원은 두 개의 가중치 값의 최적점과 그 가중치 값으로 생기는 비용함수의 공통 범위
 - 아래 마름모나 원은 가 가질 수 있는 범위이고 타원과 만나는 점이 바로 사용가능한 가중치 값
 - L1정규화는 직선과 타원 만나는 점이 양쪽 끝에 생성됨
    - 극단적인 값이 생성되어 다른 가중치 값이 선택되지 않는 현상이 발생할 수 있음
    - 사용해야 하는 피쳐와 사용하지 않아도 되는 피쳐를 선택하여 사용하도록 지원
 - L2 정규화는 원과 타원이 만나는 점이 많아져서 비교적 쉽게 연산되어 계산 효율 (computational efficiency) 확보
    - 한 점에서 만나기 때문에 하나의 해답만 제공






