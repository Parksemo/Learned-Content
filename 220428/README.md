## 머신러닝
 - 머신러닝
    - 데이터를 기반으로 기계 스스로 학습하는 인공지능의 한 분야
    - 주어진 데이터를 기반으로 기계가 스스로 학습
        성능을 향상시키거나 최적의 해답을 찾기 위한 지능적 학습 방법
    - 컴퓨터가 스스로 학습을 할 수 있도록 해주는 인공지능의 한 형태
        명시적(explicit)으로 프로그래밍이 필요 없음
    - 데이터는 매우 중요
        더 많은 데이터가 유입되면 컴퓨터는 더 많이 학습
        시간이 흐르면서 스마트해져서 작업을 수행하는 능력과 정확도가 향상
 - 데이터 수집
    - 수집된 데이터의 품질과 양에 따라
        예측 모델의 성능이 결정
    - 머신러닝 과정에서 가장 중요한 단계
 - 데이터 전처리
    - 잘못된 값은 수정하는 데이터 정리 필요
        누락 값을 채우고
        이상 값을 수정하거나 제거
    - 정규화(normalization) 과정도 필요
        데이터 특성마다 차이가 너무 크면
        머신러닝 모델에 따라 값을 비슷한 크기로 조정
    - 데이터 변환 작업이 필요
        모델의 계산에 적합한 자료구조로
        기초 자료로 새로운 자료를 생성
 - 모델 학습
    - 적절한 머신러닝 모델을 생성해 데이터로 학습
 - 성능 개선
    - 학습된 모델의 성능을 시험하고 보다 좋은 결과를 도출
 - 시각화
    - 데이터와 머신러닝 과정 그리고 예측 결과를 보기 좋게 시각화하는 과정
 - 차원 축소(dimensionality reduction)
    - 높은 차원의 원시 데이터(raw data)에서 중복되거나 관련성이 떨어지는 특성을 파악해
        차원 수를 줄이는 방식
        모델의 이해를 높이고 계산 속도를 향상
        모델의 성능을 향상시킬 수 있음
    - 데이터의 차원 축소는 정보의 손실이 발생 가능
        그러므로 데이터의 차원을 축소하면서 정보 손실을 최소로 한다면 금상첨화


## 딥러닝
### 신경망
 - 최근에 많은 인기를 끌고 있는 딥러닝(deep learning)의 시작은 1950년대부터 연구되어 온 인공 신경망(artificial neural network: ANN)
 - 인공 신경망은 생물학적인 신경망에서 영감을 받아서 만들어진 컴퓨팅 구조이다.
 - 뉴런
    - 가지돌기(dendrite)(또는 수상돌기)에 들어온 여러 신호는 하나로 통합되어
        어떤 임계 값을 초과하면 축삭돌기(axon)를 통해 이동되고 다음 신경 세포에 전달
 - 인공 신경망의 장점
    - 데이터만 주어지면 신경망은 예제로부터 배울 수 있다.
    - 몇 개의 소자가 오동작하더라도 전체적으로는 큰 문제가 발생하지 않는다
 - 인공 신경망은 딥러닝의 핵심
    - 뉴런을 사용한 논리 연산


### 퍼셉트론
 - 가장 간단한 인공 신경망 구조 중 하나로 1957년에 프랑크 로젠블라트(FrankRosenblatt)가 제안
 - 다수의 신호를 입력으로 받아 하나의 신호를 출력
 - 퍼셉트론 신호도 흐름을 만들고 정보를 앞으로 전달
 - 다만, 실제 전류와 달리 퍼셉트론 신호는 ‘흐른다/안 흐른다( 1 이나 0 )’의 두 가지 값을 가진다.
 - 1 을 ‘신호가 흐른다’, 0 을 ‘신호가 흐르지 않는다’라는 의미로 쓰인다.
 - 퍼셉트론
    - 다수 입력(input), 하나의 출력(output)
    - 가중치(weights)
        입력의 강도(중용도)를 표현
    - 스스로 학습하는 능력
        초기 가중치는 임의의 값 지정
        뉴런의 결과를 목표값과 비교
        그 차이인 오차(error)를 사용
        다시 그 다음 단계의 가중치에 반영
 - 임계값 –b
    - 가중치와 입력 곱의 합
        가중 합계(weighted sum)
        w0+ x1w1 + x2w2 + … + xnwn
    - 값이 임계값(threshold) b(=-w0)보다 크면
        뉴런은 실행(fire), 결과값이 1
    - 그렇지 않으면
        0이 결과값
 - 학습이라고 부르려면 신경망이 스스로 가중치를 자동으로 설정해주는 알고리즘이 필요하다 퍼셉트론에서도 학습 알고리즘이 존재한다.
 - 퍼셉트론은 직선 하나로 나눈 영역만 표현할 수 있다는 한계가있다.
 - 패턴 인식 측면에서 보면 퍼셉트론은 직선을 이용하여 입력 패턴을 분류하는 선형 분류자(linear classifier)의 일종이라고 말할 수 있다.
 - 뉴런은 다른 뉴런들로부터 신호를 받아서 모두 합한 후에 비선형 함수를 적용하여 출력을 계산한다.
 - 연결선은 가중치를 가지고 있고 이 가중치에 학습의 결과가 저장된다.
 - 퍼셉트론은 하나의 뉴론만을 사용한다. 다수의 입력을 받아서 하나의 신호를 출력하는 장치이다
 - 퍼셉트론은 AND나 OR 같은 논리적인 연산을 학습할 수 있었지만 XOR 연산은 학습할 수 없었다. 선형 분리 가능한 문제만 학습할 수 있었다.
 - 가장 왼쪽 줄을 입력층 , 맨 오른쪽 줄을 출력층 , 중간 줄을 은닉층
 - 은닉층의 뉴런은 (입력층이나 출력층과 달리)사람 눈에는 보이지 않는다.
 - h ( x )라는 함수가 등장했는데, 이처럼 입력 신호의 총합을 출력 신호로 변환하는 함수를 일반적으로 활성화 함수 activation function 라 한다.
 - ‘활성화’라는 이름이 말해주듯 활성화 함수는입력 신호의 총합이 활성화를 일으키는지를 정하는 역할
 - 민스키와 파퍼트는 다층 퍼셉트론을 학습시키는 알고리즘을 찾기가 아주 어려울 것이라고 예언하였다.
 - 1980년대 중반에 루멜하트와 힌튼 등은 다층 퍼셉트론을 위한 학습 알고리즘을 재발견하게 된다.
 - 다층 퍼셉트론(multilayer perceptron: MLP): 입력층과 출력층 사이에 은닉층(hiddenlayer)을 가지고 있는 신경망
 - 역전파 알고리즘은 입력이 주어지면 순방향으로 계산하여 출력을 계산한 후에 실제 출력과 우리가 원하는 출력 간의 오차를 계산
 - 이 오차를 역방향으로 전파하면서 오차를 줄이는 방향으로 가중치를 변경


### 오차 역전파
 - 역전파 알고리즘은 입력이 주어지면 순방향으로 계산하여 출력을 계산한 후에 실제 출력과 우리가 원하는 출력 간의 오차를 계산한다.
 - 이 오차를 역방향으로 전파하면서 오차를 줄이는 방향으로 가중치를 변경한다.
 - 손실함수
    - 전체 오차는 목표 출력값에서 실제 출력값을 빼서 제곱한 값을 모든 출력 노드에 대하여 합한 값이다.
 - 경사 하강법
    - 현재 위치에서 함수의 그래디언트값을 계산한 후에 그래디언트의 반대 방향으로 움직이는 방법이다.
    - 신경망의 가중치를 작은 난수로 초기화한다.
    - do 각 훈련 샘플 sample에 대하여 다음을 반복한다.
    - actual = calculate_network(sample) // 순방향 패스
    - target = desired_output(sample)
    - 각 출력 노드에서 오차(target - actual)을 계산한다.
    - 은익층에서 출력층으로의 가중치 변경값을 계산한다. // 역방향 패스
    - 입력층에서 은닉층으로의 가중치 변경값을 계산한다. // 역방향 패스
    - 전체 가중치를 업데이트한다.
    - until 모든 샘플이 올바르게 분류될 때까지
 - 계산그래프
    - 계산 그래프의 특징은 ‘국소적 계산’을 전파함으로써 최종 결과를 얻는다
 - 계산 그래프를 이용한 오차 역전파
 - 덧셈 노드의 역전파
 - 곱셈 노드의 역전파
 - 수식 wx+b에서 가중치 w를 기울기 a로 생각하면 결국 ax+b가 되어 기울기가 a, y절편이 b인 1차 함수와 같음
    - 입력 특성은 x 하나이고 w를 b로 표현
        다음과 같은 간단한 수식의 퍼셉트론
    - 입력 특성이 하나
        가중치는 기울기, 편향은 절편
 - 활성화 함수는 퍼셉트론의 최종 값의 세기를 조절하는 데 사용되며 활성화 함수 출력값은 다음 퍼셉트론의 입력으로 사용
 - 퍼셉트론에서 최종 값을 결정하는 함수
    - 퍼셉트론의 입력값과 가중치를 곱한 것에 편향을 모두 합한 값에 대한 함수
 - 활성화 함수
    - 내부에서 입력받은 데이터를 근거로 다음 계층으로 출력할 값을 결정
    - 신경망을 구성할 때 설정하며 각각의 레이어를 정의할 때 세부적인 함수를 선택
    - 은닉 계층의 활성화 함수
 - 출력 계층의 활성화 함수: 목표에 부합하는 함수를 선택
 - 단위 계단 함수(unit step function)
    - 로젠블랫의 활성화 함수
    - 가중 합계(weighted sum)
        0 이상이면 결과는 1
        0 미만이면 0
 - 항등 함수(identity function)
    - 가장 단순한 활성화 함수는 입력 그대로 출력하는 f(x)=x
 - ReLU 함수
    - 양수 부분은 항등함수와 동일, 0 이하는 0
 - 시그모이드(sigmoid) 함수와 하이퍼볼릭탄젠트(tanh) 함수
 - 다층 신경망(MLP: Multi-Layer Perceptron)
    - 인공신경망을 확장해 가로로 여러 개 연결한 층(layer)을 쌓음
    - 은닉층(hidden layer) 또는 중간층
        입력층과 출력층 사이를 여러 층으로 연결, 뉴런이 여러 개 연결되어 ‘깊은 사고’를 하는 것을 의미
        처리할 계산량이 기하급수적으로 늘어나 그만큼 정확도가 높아진 결과를 얻을 수 있음
    - 깊은 딥러닝: 여러 층의 은닉층의 단계가 많아질수록
 - 손실 함수(loss function)
    - 비용 함수(cost function)
    - 학습의 목표가 되는 기준 지표의 계산식, 목적 함수(objective function)
        오류를 최소화하는 방향으로 학습을 수행
    - 손실과 비용이라는 용어
        값이 작아질수록 목표에 접근
 - Sigmoid 계층
 - 최적화 함수
    - 손실함수의 결과값을 최소화하는 함수
    - 경사하강법(Gradient Descent)
    - 확률적 경사하강법(SGD: Stochastic Gradient Descent)
    - RMSProp
 - 경사하강법(gradient descent)
    - 말 그대로 ‘경사 따라 내려가기’ 방식
    - 기울기를 계산해 기울어진 방향으로 조금씩 이동하는 과정을 반복적으로 수행
    - 기울기가 최소가 되는 지점으로 이동하는 방식
    - 손실함수의 값이 최소가 되는 지점을 찾아 가는 방법
 - 가중치 계산
    - 초기화된 가중치는 입력이 반복됨에 따라 더욱 적합한 값을 가질 수 있도록 첫 예측값 Y와 진짜 타깃 Y의 오차를 최소화
    - 손실함수(Loss Function)로 오차를 구함
    - 최적화함수(Optimization Function)를 사용하여 조절
    - 활성화함수(Activation Function)로 계산하여 다음 입력으로 사용
    - 반복