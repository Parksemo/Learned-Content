{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a142a2d1",
   "metadata": {},
   "source": [
    "사전 학습된 한국어 BERT를 이용하여 마스크드 언어 모델을 실습해봅시다. \n",
    "이번 실습을 위해서만이 아니라 앞으로 사전 학습된 BERT를 사용할 때는 transformers라는 패키지를 자주 사용하게 됩니다. \n",
    "실습 환경에 transformers 패키지를 설치해둡시다.\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "joint-germany",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (4.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers) (0.0.53)\n",
      "Requirement already satisfied: dataclasses in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers) (4.61.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers) (4.8.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: requests in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.4.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: click in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ubuntu/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d030687",
   "metadata": {},
   "source": [
    "# 1. 마스크드 언어 모델과 토크나이저"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6752d54f",
   "metadata": {},
   "source": [
    "transformers 패키지를 사용하여 모델과 토크나이저를 로드합니다. \n",
    "BERT는 이미 누군가가 학습해둔 모델을 사용하는 것이므로 우리가 사용하는 모델과 토크나이저는 항상 맵핑 관계여야 합니다. \n",
    "예를 들어서 A라는 이름의 BERT를 사용하는데, B라는 이름의 BERT의 토크나이저를 사용하면 모델은 텍스트를 제대로 이해할 수 없습니다. \n",
    "A라는 BERT의 토크나이저는 '사과'라는 단어를 36번으로 정수 인코딩하는 반면에, B라는 BERT의 토크나이저는 '사과'라는 단어를 42번으로 정수 인코딩하는 등 단어와 맵핑되는 정수 정보 자체가 다르기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "240b848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForMaskedLM\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1845d66d",
   "metadata": {},
   "source": [
    "TFBertForMaskedLM.from_pretrained('BERT 모델 이름')을 넣으면 [MASK]라고 되어있는 단어를 맞추기 위한 마스크드 언어 모델링을 위한 구조로 BERT를 로드합니다. 다시 말해서 BERT를 마스크드 언어 모델 형태로 로드합니다.\n",
    "\n",
    "AutoTokenizer.from_pretrained('모델 이름')을 넣으면 해당 모델이 학습되었을 당시에 사용되었던 토크나이저를 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b1f15ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-large-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7d9a86c6e04d839ab7d6431217f58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "978f2ff24bac449e848e86a27914fe84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8193cdda2c9a4151a0c85c99dbda8945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = TFBertForMaskedLM.from_pretrained('bert-large-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac31210",
   "metadata": {},
   "source": [
    "# 2. BERT의 입력"
   ]
  },
  {
   "cell_type": "raw",
   "id": "37e2657f",
   "metadata": {},
   "source": [
    "''Soccer is a really fun [MASK]'라는 임의의 문장이 있다고 해봅시다. \n",
    "\n",
    "이를 마스크드 언어 모델의 입력으로 넣으면, 마스크드 언어 모델은 [MASK]의 위치에 해당하는 단어를 예측합니다. \n",
    "마스크드 언어 모델의 예측 결과를 보기위해서 bert-large-uncased의 토크나이저를 사용하여 해당 문장을 정수 인코딩해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56b228d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer('Soccer is a really fun [MASK].', return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2446396c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 101 4715 2003 1037 2428 4569  103 1012  102]], shape=(1, 9), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a283ad12",
   "metadata": {},
   "source": [
    "토크나이저로 변환된 결과에서 token_type_ids를 통해서 문장을 구분하는 세그먼트 인코딩 결과를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e60944a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0 0 0 0 0 0 0 0 0]], shape=(1, 9), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(inputs['token_type_ids'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96e22412",
   "metadata": {},
   "source": [
    "현재의 입력은 문장이 두 개가 아니라 한 개이므로 여기서는 문장 길이만큼의 0 시퀀스를 얻습니다. 만약 문장이 두 개였다면 두번째 문장이 시작되는 구간부터는 1의 시퀀스가 나오게 되지만, 여기서는 해당되지 않습니다.\n",
    "\n",
    "토크나이저로 변환된 결과에서 attention_mask를 통해서 실제 단어와 패딩 토큰을 구분하는 용도인 어텐션 마스크를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27772fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1 1 1 1 1 1 1 1 1]], shape=(1, 9), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d265183",
   "metadata": {},
   "source": [
    "현재의 입력에서는 패딩이 없으므로 여기서는 문장 길이만큼의 1 시퀀스를 얻습니다. \n",
    "만약 뒤에 패딩이 있었다면 패딩이 시작되는 구간부터는 0의 시퀀스가 나오게 되지만, 여기서는 해당되지 않습니다. \n",
    "좀 더 다양한 패턴의 입력은 뒤의 텍스트 분류, 개체명 인식, 질의 응답 실습에서 이어서 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacb9600",
   "metadata": {},
   "source": [
    "# 3. [MASK] 토큰 예측하기"
   ]
  },
  {
   "cell_type": "raw",
   "id": "076fb857",
   "metadata": {},
   "source": [
    "FillMaskPipeline은 모델과 토크나이저를 지정하면 손쉽게 마스크드 언어 모델의 예측 결과를 정리해서 보여줍니다. \n",
    "FillMaskPipeline에 우선 앞서 불러온 모델과 토크나이저를 지정해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2234f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import FillMaskPipeline\n",
    "pip = FillMaskPipeline(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9786a05",
   "metadata": {},
   "source": [
    "이제 입력 문장으로부터 [MASK]의 위치에 들어갈 수 있는 상위 5개의 후보 단어들을 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53bb6c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.7621117234230042,\n",
       "  'token': 4368,\n",
       "  'token_str': 'sport',\n",
       "  'sequence': 'soccer is a really fun sport.'},\n",
       " {'score': 0.20341990888118744,\n",
       "  'token': 2208,\n",
       "  'token_str': 'game',\n",
       "  'sequence': 'soccer is a really fun game.'},\n",
       " {'score': 0.01220858097076416,\n",
       "  'token': 2518,\n",
       "  'token_str': 'thing',\n",
       "  'sequence': 'soccer is a really fun thing.'},\n",
       " {'score': 0.0018630382837727666,\n",
       "  'token': 4023,\n",
       "  'token_str': 'activity',\n",
       "  'sequence': 'soccer is a really fun activity.'},\n",
       " {'score': 0.0013354948023334146,\n",
       "  'token': 2492,\n",
       "  'token_str': 'field',\n",
       "  'sequence': 'soccer is a really fun field.'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip('Soccer is a really fun [MASK].')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e800faf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.25628983974456787,\n",
       "  'token': 2265,\n",
       "  'token_str': 'show',\n",
       "  'sequence': 'the avengers is a really fun show.'},\n",
       " {'score': 0.1728411316871643,\n",
       "  'token': 3185,\n",
       "  'token_str': 'movie',\n",
       "  'sequence': 'the avengers is a really fun movie.'},\n",
       " {'score': 0.11107711493968964,\n",
       "  'token': 2466,\n",
       "  'token_str': 'story',\n",
       "  'sequence': 'the avengers is a really fun story.'},\n",
       " {'score': 0.07248979061841965,\n",
       "  'token': 2186,\n",
       "  'token_str': 'series',\n",
       "  'sequence': 'the avengers is a really fun series.'},\n",
       " {'score': 0.07046633213758469,\n",
       "  'token': 2143,\n",
       "  'token_str': 'film',\n",
       "  'sequence': 'the avengers is a really fun film.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip('The Avengers is a really fun [MASK].')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9df19b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.35730716586112976,\n",
       "  'token': 2147,\n",
       "  'token_str': 'work',\n",
       "  'sequence': 'i went to work this morning.'},\n",
       " {'score': 0.23304452002048492,\n",
       "  'token': 2793,\n",
       "  'token_str': 'bed',\n",
       "  'sequence': 'i went to bed this morning.'},\n",
       " {'score': 0.12845063209533691,\n",
       "  'token': 2082,\n",
       "  'token_str': 'school',\n",
       "  'sequence': 'i went to school this morning.'},\n",
       " {'score': 0.062305696308612823,\n",
       "  'token': 3637,\n",
       "  'token_str': 'sleep',\n",
       "  'sequence': 'i went to sleep this morning.'},\n",
       " {'score': 0.04695264995098114,\n",
       "  'token': 2465,\n",
       "  'token_str': 'class',\n",
       "  'sequence': 'i went to class this morning.'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip('I went to [MASK] this morning.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbd19f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
