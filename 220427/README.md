## 앙상블
 - 대중적인 데이터 분석 알고리즘
 - 최근 머신러닝/딥러닝 분야에서 딥러닝 다음으로 부스팅(boosting) 알고리즘이 핵심적으로 사용됨
 - 선형회귀나 로지스틱 회귀는 가장 대중적인 알고리즘이고, 그 다음이 의사결정트리와 앙상블 계열 알고리즘, 딥러닝
 - 앙상블(ensemble)
    - 여러 개의 알고리즘들이 하나의 값을 예측하는 기법을 통칭하여 말함
    - 회귀 문제에서는 가중 평균이나 단순 평균을 구하는 방식으로 Y 값을 예측
    - 메타 분류기(meta-classifier)라고도 부름
        메타(meta)는 일종의 상위 또는 추상화라는 개념.
        여러 분류기들을 모아 하나의 분류기를 만들어 이를 메타 분류기라고 부른다
 - 시간이 굉장히 오래 걸리지만 비교적 좋은 성능을 냄
 - 하나의 데이터를 넣음 → 이를 여러 모델에 학습시키고 → 테스트 데이터를 각 모델에 입력 → 투표 또는 여러 가중치 기법을 적용하여 최종 선택
 - 앙상블 기법들
 - 바닐라 앙상블
    - 가장 기본적인 앙상블 기법. 바닐라라고 하면 아이스크림에서 아무것도 첨가되지 않은 맛인데, 바닐라 앙상블도 아무것도 처리하지 않은 앙상블 모델을 의미.
    - 일반적으로 가중치 평균이나 투표 방식으로 만들어지는 앙상블 모델
 - 부스팅
    - 하나의 모델에서 여러 데이터를 샘플링한 다음 그 샘플링된 데이터로 각각의 모델을 만드는 기법
 - 배깅
    - ‘boosting aggregation(부스팅 집합)’의 줄임말로 부스팅을 좀 더 발전시킨 기법
 - 투표 분류기(voting classifier)
    - 여러 개의 모델을 만들어 모두 같은 데이터를 넣고 결과를 취합하여 가장 많이 선택된 결과를 취함
 - 앙상블 모델의 가장 기본적인 형태
 - 다수결 분류기(majority voting classifier)라고도 부름
 - 또는 각 분류기마다 가중치를 주고 해당 가중치를 각 모델에 곱하여 가중치의 합을 구하는 방식
 - 장점 : 다양한 모델을 만든 후, 다음 단계로 매우 쉽게 만들 수 있음
 - 배깅(bagging)
    - 하나의 데이터셋에서 샘플링을 통해 여러 개의 데이터셋을 만든 다음 각 데이터셋마다 모델을 개발하여 투표 분류기로 만드는 기법
    - 단순하면서 성능이 높아 특히 트리 계열 알고리즘과 함께 많이 사용되며 통계적인 샘플링 기법이나 딥러닝 기법과도 함께 사용
 - 샘플링(sampling)
    - 다루고자 하는 데이터가 전체 모수라면 그 모수에서 일부분을 뽑아서 데이터를 분석
 - 배깅의 장점
    - 다양한 데이터셋에서 강건한 모델(robust model)을 개발할 수 있다
 - 약분류기와 강분류기로 알아보는 배깅 기법
 - 배깅 기법은 여러 개의 약분류기(weak learner)로 강분류기(strong learner)를 만드는 것이다.
 - 약분류기는 기본적으로 과소적합이 다소 있지만 과적합되어 있지 않은 모델. 다소 느슨하게 경계를 생성하는 여러 개의 약한 분류기를 앙상블한다면 좀 더 정확한 경계를 생성한다.
 - 각각의 작은 데이터로는 모든 구체적인 분류 영역을 정할 수 없지만 많은 데이터로 투표한다면 더 높은 성능을 기대할 수 있다.
 - 부트스트래핑(bootstrapping)
    - 모수 데이터로부터 학습 데이터를 추출할 때 임의의 데이터를 추출한 후 복원추출하는 여러번의 과정
 - 복원추출
    - 전체 데이터에서 먼저 일부를 추출하여 이를 ‘학습 데이터셋 1’이라고 부른 다음 다시 그 데이터를 모수에 집어넣고 ‘학습 데이터 셋 2’를 뽑는 방식
 - ‘.632 부트스트래핑’ 기법
    - 전체 데이터 S에서 n번의 데이터를 추출할 때, 이를 많이 추출할수록 각 데이터가 나타날 확률이 63.2%에 가까워짐
 - 배깅(bagging)은 부트스트랩 집합이라는 의미의 ‘bootstrap aggregation’의 약자로, 말그대로 부트스트랩 연산의 집합이라는 개념
 - 데이터셋으로부터 부분집합 n개를 추출 → 앙상블 방법과 달리 하나의 모델에 다양한 데이터셋을 넣어서 n개의 모델을 생성
 - 높은 분산으로, 일반적인 모델로 만들 경우 과적합이 심한 데이터셋에 좀 더 강건
 - 각 모델들은 해당 데이터셋에 맞춰진 과적합 모델
 - Out-of-bag Error
    - 배깅 모델의 성능을 측정하기 위해서 정확도나 정밀도 외에 ‘Out-of-bag Error’라는 지표를 사용한다.
    - 일반적으로 ‘OOB error estimation’이라고 부른다.
    - 배깅에서 부분집합을 생성할 때 일부 데이터만 학습에 사용되는데, 각 부분집합에서 학습에 사용되지 않은 데이터셋에 대해서만 성능을 측정하여 배깅 모델의 효과를 측정하는 것이다.
    - 기본적으로 검증셋(validation set)과 유사한 방식으로 학습에 사용하지 않은 데이터를 가지고 학습의 성능을 측정한다고 이해할 수 있다
 - 랜덤 포레스트(random forest)
    - 하나의 모델을 나무라고 한다면 이러한 나무들을 이용해 랜덤하게 데이터를 뽑아서 숲을 생성하는 알고리즘
 - 배깅 알고리즘을 의사결정트리에 적용한 모델
 - 사이킷런 배깅 분류기 BaggingClassifier
    - base_estimator : 사용될 수 있는 모델(default=None)
    - n_estimators : int, optional(default=10), subset으로 생성되는 모델의 개수
    - max_samples : int or float, optional(default=1.0), 최대 데이터 개수 또는 비율
    - max_features : int or float, optional(default=1.0), 최대 사용 피쳐 또는 비율
    - bootstrap : boolean, optional(default=True), bootstrap 사용 여부
    - oob_score : boolean, oob score 산출 여부
    - warm_start : booeanl, optional(default=False), 이전에 학습된 모델을 사용할 것인가에 대한 정보
 - 부스팅(boosting)
    - 학습 라운드를 차례로 진행하면서 각 예측이 틀린 데이터에 점점 가중치를 주는 방식
    - 라운드별로 잘못 분류된 데이터를 좀 더 잘 분류하는 모델로 만들어 최종적으로 모델들의 앙상블을 만드는 방식
    - 배깅 알고리즘이 처음 성능을 측정하기 위한 기준(baseline) 알고리즘으로 많이 사용된다면, 부스팅 알고리즘은 높은 성능을 내야 하는 상황에서 가장 좋은 선택지
 - 틀린 부분만 집중해서 모델들을 순차적으로 만들고, 해당 모델들은 최종적으로 앙상블
 - 배깅과 부스팅의 차이점
    - 병렬화 가능 여부
    - 기준 추정치
    - 성능 차이
 - 병렬화 가능 여부
    - 배깅은 데이터가 n개라면 n개의 CPU로 한번에 처리하도록 구조를 설계할 수 있음
    - 배깅은 데이터를 나눠 데이터마다 조금씩 다른 모델을 생성
    - 부스팅은 단계적으로 모델들을 생성하고 해당 모델들의 성능을 측정한 후 다음 단계로 넘어가 병렬화를 지원하지 않음
    - 부스팅은 배깅에 비해 속도가 매우 떨어짐
 - 기준 추정치
    - 배깅 개별 모델들은 높은 과대적합으로 모델의 분산이 높음
    - 부스팅은 각각의 모델에 편향이 높은 기준 추정치를 사용하여 개별 모델들은 과소적합이 발생하지만 전체적으로 높은 성능을 낼 수 있는 방향으로 학습
    - 부스팅 모델의 이러한 특징을 약한 학습자(weak learner)라고 부름
 - 성능 차이
    - 부스팅은 기본적으로 비용이 높은 알고리즘
        비용은 속도나 시간을 말함
    - 배깅은 데이터의 부분집합에 대해 학습을 수행하기 때문에 부스팅보다 좋은 성능을 내기는 어려움
    - 초기 성능을 측정할 때는 배깅, 이후의 성능 측정은 부스팅으로 하는 것이 가장 일반적인 접근
 - 에이다부스트(AdaBoost)
    - 매 라운드마다 인스턴스, 즉 개별 데이터의 가중치를 계산하는 방식
    - 부스팅 알고리즘 중 대표적인 알고리즘
    - 매 라운드마다 틀린 값이 존재하고 해당 인스턴스에 가중치를 추가로 주어 가중치를 기준으로 재샘플링(resampling)
 - 스텀프(stump)
    - ‘그루터기’라는 뜻으로 나무의 윗부분을 자르고 아랫부분만 남은 상태
    - 에이다부스트에서 스텀프는 학습할 때 큰 나무를 사용하여 학습하는 것이 아니라 나무의 그루터기만을 사용하여 학습한다는 개념
    - 1뎁스(depth) 또는 2뎁스 정도의 매우 간단한 모델을 여러 개 만들어 학습한 후, 해당 모델들의 성능을 에이다부스트 알고리즘을 적용하여 학습하는 형태


## K-평균 군집화
 - k-Nearest Neighbor(kNN) 알고리즘
    - 모든 기계 학습 알고리즘 중에서도 가장 간단하고 이해하기 쉬운 분류 알고리즘
 - 군집(clustering)
    - 비슷한 샘플을 클러스터(cluster)로 모음
    - 비슷한 샘플을 구별해 하나의 클러스터 또는 비슷한 샘플의 그룹으로 할당하는 작업 군집은 다양한 애플리케이션에서 사용
    - 데이터 분석, 고객 분류, 추천 시스템, 검색 엔진, 이미지 분할, 준지도 학습, 차원 축소 등에 사용
 - 이상치 탐지(outlier detection)
    - ‘정상’ 데이터가 어떻게 보이는지를 학습하고, 비정상 샘플을 감지하는 데 사용
    - 예를 들면 제조 라인에서 결함 제품을 감지하거나 시계열 데이터에서 새로운 트렌드를 찾는다.
 - 밀도 추정(density estimation)
    - 데이터셋 생성 확률 과정(random process)의 확률 밀도 함수(probability density function, PDF)를 추정한다.
    - 밀도 추정은 이상치 탐지에 널리 사용된다.
    - 밀도가 매우 낮은 영역에 놓인 샘플이 이상치일 가능성이 높다.
    - 데이터 분석과 시각화에도 유용하다.
 - 비지도 학습 중에서 가장 대표적인 것이 K-means 알고리즘이다.
 - K-means 알고리즘(K-means algorithm)은 주어진 n개의 관측값을 k개의 클러스터로 분할하는 알고리즘으로, 관측값들은 거리가 최소인 클러스터로 분류된다.
 - K-평균(k-Means) 방식
    - 비지도학습의 대표적인 알고리즘
    - K-평균 클러스터 방식
    - 출력 데이터 없이 입력만을 가지고 학습하여 결과(라벨)를 생성
 - 입력값
1. k: 클러스터 수
2. D: n개의 데이터
 - 출력값: k개의 클러스터
 - 알고리즘
1. 집합 D에서 k개의 데이터를 임의로 추출하고, 이 데이터들을 각 클러스터의 중심 (centroid) 으로 설정한다. (초기값 설정)
2. 집합 D의 각 데이터에 대해 k개의 클러스터 중심과의 거리를 계산하고, 각 데이터가 어느 중심점 (centroid)와 가장 유사도가 높은지 알아낸다. 그리고 그렇게 찾아낸 중심점으로 각 데이터들을 할당한다.
3. 클러스터의 중심점을 다시 계산한다. 즉, 2에서 재할당된 클러스터들을 기준으로 중심점을 다시 계산한다.
4. 각 데이터의 소속 클러스터가 바뀌지 않을 때까지 과정 2, 3을 반복한다.
 - k-평균
    - 반복 몇 번으로 레이블이 없는 데이터셋을 빠르고 효율적으로 클러스터로 묶는 간단한 알고리즘
    - 프로토타입 기반 군집(prototype-based clustering)에 속함
    - k-평균 알고리즘이 원형 클러스터를 구분하는 데 뛰어나지만, 이 알고리즘 단점은 사전에 클러스터 개수 k를 지정해야 하는 것
    - 적절하지 않은 k를 고르면 군집 성능이 좋지 않음
    - 나중에 군집 품질을 평가하는 기법 엘보우 방법(elbow method)과 실루엣 그래프(silhouetteplot)
 - 프로토타입 기반 군집
    - 각 클러스터가 하나의 프로토타입으로 표현된다는 뜻
    - 연속적인 특성에서는 비슷한 데이터 포인트의 센트로이드(centroid)(평균)이거나
    - 범주형 특성에서는 메도이드(medoid)(가장 대표되는 포인트나 가장 자주 등장하는 포인트)가 됨
 - "팔꿈치" 방법(elbow method)에서는 k를 1부터 증가시키면서 K-means 클러스터링을 수행한다. 각 k의 값에 대하여 SSE(sum of squared errors)의 값을 계산한다.