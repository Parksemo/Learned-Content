## 머신러닝 딥러닝 기초
 - 리지 회귀(ridge regression)
    - L2 정규화(L2 regularization)
    - 놈(norm)
    - 좌표평면의 원점에서 점까지의 거리를 나타내어 벡터의 크기를 측정하는 기법
    - X는 하나의 벡터
    - L2 놈(L2 norm) : 벡터 각 원소들의 제곱합에 제곱근을 취함
 - 리지 회귀는 L2 놈을 선형회귀의 비용함수 수식에 적용
 - 뒷부분에 새로 붙인 수식은 페널티텀(penalty term)으로, 모델의 가중치 값들의 제곱의 합
    - 가중치 값이 조금이라도 커질 때 비용함수에 매우 큰 영향
    - λ가 클수록 전체 페널티텀의 값이 커져 θ 값이 조절됨
    - λ는 사람이 직접 값을 입력하는 하이퍼 매개변수
 - 리지 회귀 수식을 미분하면 j의 값이 1 이상일 때 페널티가 적용됨
 - 라쏘 회귀(lasso regression)
    - L1 정규화(L1 regularization)라고 부름
    - 가중치에 페널티텀을 추가하는데, 기존 수식에다 L1 놈 페널티를 추가하여 계산
 - L1 놈(L1 norm) : 절대값을 사용하여 거리를 측정
 - L1 정규화와 L2 정규화가 실제 적용되는 과정
 - 타원은 두 개의 가중치 값의 최적점과 그 가중치 값으로 생기는 비용함수의 공통 범위
 - 아래 마름모나 원은 가 가질 수 있는 범위이고 타원과 만나는 점이 바로 사용가능한 가중치 값
 - L1정규화는 직선과 타원 만나는 점이 양쪽 끝에 생성됨
    - 극단적인 값이 생성되어 다른 가중치 값이 선택되지 않는 현상이 발생할 수 있음
    - 사용해야 하는 피쳐와 사용하지 않아도 되는 피쳐를 선택하여 사용하도록 지원
 - L2 정규화는 원과 타원이 만나는 점이 많아져서 비교적 쉽게 연산되어 계산 효율 (computational efficiency) 확보
    - 한 점에서 만나기 때문에 하나의 해답만 제공
 - 다중 선형 회귀
    - 여러 개의 특성이 있는 경우로 일반화 한 회귀
    - 여기서 w0는 y축의 절편이고 x0=1
    - 단변량 회귀와 다변량 회귀는 같은 개념과 평가 기법을 사용
 - 회귀설계
1. 데이터 확보
2. 데이터 전처리
3. 데이터 분류
4. 데이터 학습
5. 예측하기와 결과 분석
 - 데이터 확보
    - sklearn.datasets 라이브러리 load_boston 모듈을 사용하여 데이터를 추출
    - 딕셔너리 타입의 객체를 반환
    - data 키 값 추출
    - x와 y 각 데이터셋을 추출
    - y_data는 n×1의 형태로 변환
 - 데이터 전처리하기
    - 피쳐 스케일링 적용
 - 데이터 분류
    - 데이터를 훈련과 테스트 형태로 분류
 - 데이터 학습하기
    - 학습에 사용할 알고리즘 해당하는 모델의 클래스 호출
        각 클래스의 매개변수를 이해해야 함
 - 공통적으로 사용하는 매개변수
    - fit_intercept : 절편을 사용할지 말지를 선택
    - normalize : 학습할 때 값들을 정규화할지 말지
    - copy_X : 학습 시 데이터를 복사한 후 학습을 할지 결정
    - n_jobs : 연산을 위해 몇 개의 CPU를 사용할지 결정
    - alpha : 라쏘 회귀, 리지 회귀, SGD에 있음. 페널티 값을 지정
 - SGD의 매개변수
    - 직접 penalty 함수를 지정할 수 있는데, λ 값을 alpha에 입력
    - max_iter : 최대 반복 횟수를 지정
    - tol : 더 이상 비용이 줄어들지 않을 때 반복이 멈추는 최솟값
    - eta0 : 한 번에 실행되는 학습률
 - 사이킷런은 ‘적합-예측(fit-predict)’ 또는 ‘적합-변형(fit-transform)’의 구조
    - 모델을 생성한 후 예측을 하거나 전처리 모델의 규칙을 세운 후 데이터 전처리를 적용하는 구조
 - 예측하기와 결과 분석
    - 만들어진 함수로 실제 예측을 한다
    - 사이킷런에서 지표들(metrics)을 호출하여 성능을 비교
    - 필요에 따라 시각화 도구로 예측값과 실제값 비교


### 로지스틱 회귀
 - 분류 문제
    - 몇 가지 이산적 값 중 하나를 선택하는 모델. ‘분류 모델’이라고 부름
 - 초록색 선을 추가해 선 상단은 합격, 선 하단은 불합격
 - 아래 수식으로 기존 선형회귀 모델을 적용
 - 문제점:
1. f(x)의 값이 1 이상이나 0 이하로 나올 수 있음
2. 각 피쳐들이 Y에 영향을 주는 것을 해석하는 문제
3. 사건의 발생 여부는 이산적인데 실제 수식은 연속적
 - 로지스틱 회귀의 개념
    - 이진 분류(binary classification) 문제를 확률로 표현
    - 어떤 사건이 일어날 확률을 P(X)로 나타내고 일어나지 않을 확률을 1 - P(x)로 나타냄 (0 ≤ P(X) ≤ 1)
    - 오즈비(odds ratio) : 어떤 사건이 일어날 확률과 일어나지 않을 확률의 비율
 - 확률이 올라갈수록 오즈비도 급속히 상승
 - 로짓(logit) 함수 : 오즈비에 상용로그를 붙인 수식
 - X 값으로 확률을 넣으면 logit(P) 꼴로 나타남
 - 확률을 구하려면 기존 함수의 역함수를 취하여 연산
 - 로지스틱 함수(logistic function)
    - 로짓 함수의 역함수
 - 그래프가 S자 커브 형태인 시그모이드 함수(sigmoid function)
 - 로지스틱 회귀(Logistic Regression)
    - 종속변수가 이분형일 때 수행할 수 있는, 예측 분석을 위한 회귀분석 기법
 - 시그모이드 함수 수식
    - y 값을 확률 p로 표현
    - z 값은 선형회귀와 같이 가중치와 피쳐의 선형 결합(linear combination)으로 표현 가능